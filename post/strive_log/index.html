<!DOCTYPE html>
<html lang="en-us">
<title>Strive School log | Gavaskar blog</title>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.91.2" />
<meta name="description" content="my blog">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="https://gawasinspire.github.io/css/index.css">
<link rel="canonical" href="https://gawasinspire.github.io/post/strive_log/">
<link rel="alternate" type="application/rss+xml" href="" title="Gavaskar blog">

<header>
  
    <a href="https://gawasinspire.github.io/" class="title">Gavaskar blog</a>
  
  
    <nav>
    
      <a href="/about/">About</a>
    
    </nav>
  
</header>

<article>
  <header>
    <h1>Strive School log</h1>
    <time datetime="2020-12-21T21:34:22&#43;01:00">December 21, 2020</time>
  </header>
  <p>links and hyperlinks not properly linked need to fix after uploads</p>
<h1 id="strive-logs">Strive logs</h1>
<p>Hello all, this is my(Gavaskar) log book that i used during strive school for reference.
Non-stop coding enjoyment..+</p>
<p>Ideas:</p>
<p>OCR —&gt; latex</p>
<p>Chess with image—&gt; multi threaded, npm solver.</p>
<p>Strive: make great efforts to achieve or obtain something that you thought it was impossible.</p>
<p><img src="content/Untitled.png" alt="content/Untitled.png"></p>
<p>Day 1:</p>
<p>Intro and advertisements about Strive schools. Little bored</p>
<p>I have upgraded my KDE with plasma.</p>
<p><img src="content/Untitled%201.png" alt="content/Untitled%201.png"></p>
<p>Day 2:</p>
<p>All system/environment setup</p>
<p>installed conda + zsh + jupyter notebooks</p>
<p>learned the power of google colab 👏</p>
<p><a href="http://colab.research.google.com/">Google Colaboratory</a></p>
<p>fixed some problems with zsh and conda</p>
<p>did my eduflow day targets. (easy)</p>
<p>introduced with <a href="http://loom.com">loom.com</a>, a video recorder seams cool. 😎</p>
<p>Day 3:</p>
<p>raised an issue for helping fellow students with solutions to fix it. 💪</p>
<p>learned conda environments setting ups&hellip; yml</p>
<p>spent lot of time in searching Ex. 5.3 but not available. lol.</p>
<p>after so many confusions and struggles, meet deadlines yea&hellip;💪</p>
<p>In between, there was a HR speaker revealed tips and tricks to hack interviews. 👏 thanks @valeria-pop-oz</p>
<p>Day 4:</p>
<p>I have appraised by fellow mate for a little help and felt shy. Irritating errors on Eduflow. The class was boring. By mistake, the instructor submitted an exercise with solution. Even though i also did wrong submission on 11.2, did not go through the complete questions. My mistake.. have to work on my disciplines. scheduled to do the optional exercises from today on weekends. After strive, learned about Bait and hook business model. Attended a lecture on Business Innovation as process by Jakub from Denmark and got connected with him. That was interesting.</p>
<p>. Business model generation canvas</p>
<p>. Business model Navigator</p>
<p>Day 5:</p>
<p>Willing to learn with energetic vibes .. :) Good Morning. cool. guys fixed that bug in app.strive.school. 👏</p>
<p>Need to improve my algorithmic skills and should work on problem solving skills. how am i scheduled/plan to o that.?</p>
<ul>
<li>Need to practice more hackerrank and</li>
<li>Need to give a glance on the book suggested by my classmate at first</li>
</ul>
<p>then, think about further. deadline: this weekend.</p>
<p><a href="https://www.manning.com/books/grokking-algorithms">Grokking Algorithms</a></p>
<p><a href="https://www.manning.com/books/grokking-artificial-intelligence-algorithms?query=groking%20algorithm%20i">Grokking Artificial Intelligence Algorithms</a></p>
<p>so bought those books. 😇 need to read 🥶</p>
<p>solved today`s task not challenging. but, its okay daily routine. Learned git rebase and so on..</p>
<p>Day 6:</p>
<p>not read the algorithms books.</p>
<p>Numpy &hellip; 100 exercise →</p>
<p>committed only 60 and 40 more to go</p>
<p>Day 7:</p>
<p>pandas 100 exercise, but i completed only 60, 40 more to go</p>
<p>Day 8:</p>
<p>Web scraping :</p>
<p>Likely an apocryphal quote from Michelangelo, when asked how he sculpted something as masterful as David:</p>
<p>&ldquo;It is easy. You just chip away the stone that doesn&rsquo;t look like David&rdquo;</p>
<p>But. I felt horrible 😭 in-efficient code ever. Increased my backlogs:</p>
<p>Need to work on python numpy</p>
<p>Need to work on python pandas</p>
<p>Need to work on python webscraping&hellip;practise , practise, practise..</p>
<p>Day 9:</p>
<p>No Code day. let us see 😉 feeling excited</p>
<p>Data : visualization</p>
<p><a href="https://www.graphext.com/">https://www.graphext.com/</a></p>
<p>Forms:
typeforms</p>
<p>google forms</p>
<p>websites:    google sites, squarespaces, webflow, wordpress</p>
<p><a href="https://www.workstreams.ai/">https://www.workstreams.ai/</a></p>
<p><a href="https://www.makerpad.co/tool-directory">https://www.makerpad.co/tool-directory</a></p>
<p><a href="https://www.figma.com/">https://www.figma.com/</a></p>
<p><a href="https://webflow.com/">https://webflow.com/</a></p>
<p>Workflow</p>
<p><a href="https://www.bravostudio.app/">https://www.bravostudio.app/</a></p>
<p><a href="https://teachablemachine.withgoogle.com/">https://teachablemachine.withgoogle.com/</a></p>
<p>False profiles:</p>
<p><a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a></p>
<p>Day 10:</p>
<p>This helped me alot: <a href="https://blog.michaelyin.info/scrapy-exercises-make-you-prepared-for-web-scraping-challenge/">https://blog.michaelyin.info/scrapy-exercises-make-you-prepared-for-web-scraping-challenge/</a></p>
<p>start from here → <a href="https://scrapingclub.com/blog/scrapy-tutorial-1-scrapy-vs-beautiful-soup/">https://scrapingclub.com/blog/scrapy-tutorial-1-scrapy-vs-beautiful-soup/</a></p>
<p><a href="https://scrapingclub.com/blog/">https://scrapingclub.com/blog/</a></p>
<p>Holiday work:</p>
<p>Have a note on #Earth Hour</p>
<p><img src="content/Untitled%202.png" alt="content/Untitled%202.png"></p>
<p>need to clean this —&gt;</p>
<p><a href="http://localhost:8888/notebooks/ai/strive/git_strive/StriveSchool/day8/selenium-firefox/ex00_1.ipynb">http://localhost:8888/notebooks/ai/strive/git_strive/StriveSchool/day8/selenium-firefox/ex00_1.ipynb</a></p>
<p>wrote exam and cleared that. some questions were not clear. 😂</p>
<p>Day 11:</p>
<p><img src="content/M2.jpg" alt="content/M2.jpg"></p>
<p>Linear algebra</p>
<p><a href="https://www.geogebra.org/">https://www.geogebra.org/</a></p>
<p>recalling linear algebra:) 3blue1brown</p>
<p><a href="https://youtu.be/LyGKycYT2v0">Dot products and duality | Essence of linear algebra, chapter 9</a></p>
<p><a href="http://www.astronomia.edu.uy/progs/algebra/Linear_Algebra,_4th_Edition__(2009)Lipschutz-Lipson.pdf"></a></p>
<p>Day 12:</p>
<p>Linear algebra 2: recall and finishing back logs from yesterday..</p>
<p>exponential vs logarithmic scale.</p>
<p><img src="content/Untitled%203.png" alt="content/Untitled%203.png"></p>
<p>Day 13:</p>
<p>recalling Calculus. Why derivative..? d/dx and so on ..</p>
<p>live coding.. much better 🧧</p>
<p><a href="https://www.derivative-calculator.net/">Derivative Calculator</a></p>
<p>No deadlines today 😭</p>
<p>Day 14:</p>
<p>probability recalling..</p>
<p>Had one more Lecture on complexity.  Big O notation..and so on.</p>
<p>Day 15:</p>
<p>statistics recalling&hellip; mean, mean, median .. mode</p>
<p>Day 16:</p>
<p>Advanced Statistics..</p>
<p>Day 17:</p>
<pre><code> P-value, Here starts confusion.. No No, I am learning new things actually.. 
</code></pre>
<p>Confidence Interval:</p>
<p>We will be taking a sample from a true population, then we will again calculate sample mean, median, and mode. Now, the sample MMM lies in between range of Confidence Interval ( CI ) with Actual MMM (Mean, Median, Mode). CI depends on variance and sample size. Sampling error is reduced by having larger sample</p>
<p><a href="https://youtu.be/tFWsuO9f74o">Understanding Confidence Intervals: Statistics Help</a></p>
<p>Zc =  (X` - mu0)/(s/sqrt(n))</p>
<p><a href="https://youtu.be/8Aw45HN5lnA">P-Value Method For Hypothesis Testing</a></p>
<p>Today i could not able to finish the exercises before the dead line.. 😵 😭</p>
<p>Day 18:</p>
<p>Worked On the Hypothesis testing.. The code way.. 💪</p>
<p>Also got to know about several other ways to make pythonic Hypothesis testing statistically. 😎</p>
<p>git link..</p>
<p>Day 19:</p>
<p>Day 20:
Hypothesis Testing:</p>
<p>Day 21:</p>
<p>Build week: Scraping datas, scraped 100 datas. Learned social skills.<br>
Day 22:</p>
<p>Troubleshooted problems in scraper and scraping 1000 datas..</p>
<p>Day 23:</p>
<p>Did &ldquo;Exploratory Data Analysis&rdquo;.</p>
<p>Day 24:</p>
<p>How to present and optimize the code. How to make it more pythonic ?</p>
<p>Day 25:</p>
<p>Deploy .. Deploy&hellip; Deploy</p>
<p><a href="https://redketchup.io/icon-converter">Icon Converter | Convert Image to ICO File Online - RedKetchup</a></p>
<p><a href="https://replit.com/@PySimpleGUI">PySimpleGUI ( PySimpleGUI)</a></p>
<p>Day 26:</p>
<p>After a furious build week, starting with ML(preprocess—&gt; select—&gt; train—&gt;fit it—&gt;predict), hey today what something different &ldquo;assert&rdquo;. Ha it checks for the error. Interesting. Could not able to submit the today`s exercise but sounds good for me.</p>
<p>Want to learn one hot encoding?</p>
<p><img src="content/Untitled%204.png" alt="content/Untitled%204.png"></p>
<p>Day 27:</p>
<p>Learned Linear Regression and what is machine Learning Pipelines.</p>
<p>Day 28:</p>
<p>Unsupervised Learning.</p>
<p>Dimentionality Reductions:—&gt; PCA vs LDA :</p>
<ul>
<li>PCA looks at the categories with most Variation. (Maximize Variation)</li>
<li>LDA tries to maximize the separation of known categories. (Maximizing the component axes for class-separation)</li>
</ul>
<p>I am not a doctor or nor a professional Diagnostic but i feel i could find out if the doctor lies about breast cancer.</p>
<p>useful link: —&gt; also git</p>
<p><a href="https://www.kaggle.com/abdulmeral/breast-cancer-classification-knn-pca-nca">Breast Cancer Classification (KNN - PCA - NCA)</a></p>
<p>Day 29:</p>
<p>Day 30:</p>
<p>Day 31:</p>
<p>Logistic Regression, solved exercises from Nick Becker</p>
<p><a href="https://beckernick.github.io/logistic-regression-from-scratch/">Logistic Regression from Scratch in Python</a></p>
<p>Day 32:</p>
<p>solved KNN  before 4 hours the deadline. the Next exercise includes some complex question structured. Bored to fix that.</p>
<p>A meme from a tutor bothered me:</p>
<p>how I imagine you when you have the camera off: <a href="https://twitter.com/007/status/1387043865895620608?s=21">https://twitter.com/007/status/1387043865895620608?s=21</a></p>
<p>My Reply would be:</p>
<p>Myself, am not feeling comfortable, It is not giving me a feel of listening to classroom. 30 people seeing me all time (when in class room a fellow student will see me but i know that ).. may be i am not used to public speaking. May be i am some what introvert but i am not sure. I could share here a virtual meeting recently where i felt so comfortable. And it was so effective in my opinion. but, I wish to be unnoticed as long as it is not mandatory.</p>
<p>I find it quite informative. These kind of structure for an educative platforms. Instead of normal 1 to n video clients.</p>
<p><img src="content/Talk_Nocode_AI.png" alt="content/Talk_Nocode_AI.png"></p>
<p>Day 33:</p>
<p>Today, we played with decision trees and gradient booting algorithms. No nbgrader. exercises are also not given. just need to work out from the official document.</p>
<p>Some peers did cross validations which was amazing..</p>
<p>Day 34:
Today was completely about SVM support vector machines and kernel tricks inside that. Now i am thinking. Which ML for which dataset? Random forest works good in remote sensing and GPS.</p>
<p>Learned about grid search cv and it is computationally demanding. Grid search cv vs Random search cv —&gt; better(random search cv) optimizing</p>
<p>Yea..&ldquo;Hyper parameter tuning&rdquo;.</p>
<p>Day 35:</p>
<p>Self milestones: (outside school) Need to play with torch , transformations and pipelines</p>
<p>Kaggle Competitions..Oh ye Titanic reached top 3%. oh je..</p>
<p>With reference to syllabus one peer asked our tutor about classes on time series. The tuttor replied to teach about time series in DL module.</p>
<p>Day 36: New tutor..</p>
<p>Transformers and pipelines.. downgraded my python to 3.8..</p>
<p>very interesting day.  Did more cross validations</p>
<p>Day 37:</p>
<p>Data cleaning and preprocessing.Exercises were easy. but solved kaggle exercises.</p>
<p>Fought with Anomaly detection. Learned new concepts like Typos. Fuzzywuzzy package..</p>
<p>Day 38:</p>
<p>Got more exercises to do.. Jea.</p>
<p>What is Markov models, baum welch algorithm?</p>
<p><a href="https://medium.com/startup-grind/39-machine-learning-resources-that-will-help-you-in-every-essential-step-b2696515ed9">39 Machine Learning Resources That Will Help You in Every Essential Step</a></p>
<p>Day 39: solved Kaggle feb. Transformers and Pipelines</p>
<p>Day 40: Heart diesease UCI datasets.</p>
<p>Deployed some streamlit web pages. with some cool musics.</p>
<p>Day 41:</p>
<p>deep Feature engineering. Feature engineering is obsolete when we have sufficient dataset for neural network. Today need to do some feature engineering with performance tuning on gradient boosting..But, i worked with neural network and got better results. Kaggle competitions. Willing to know how much i could boost. and where i could improve my accuracy..?</p>
<p>Day 42: Today learning many new stuffs. 💪🏻</p>
<p>Optuna: Today all about optuna.. Before digging into that I need to know about  TPE</p>
<p>The Tree-structured Parzen Estimator (TPE) is a sequential model-based optimization (SMBO) approach. SMBO methods sequentially construct models to approximate the performance of hyperparameters based on historical measurements, and then subsequently choose new hyperparameters to test based on this model.</p>
<p>TPE is one of the Bayesian Methods.</p>
<p>In hyperperformance tuning, We have options like</p>
<p>Grid search—&gt; Grid type;          Random search —&gt; random type</p>
<p>Independent TPE—&gt;Bayesian ; Multivariate TPE—&gt; Bayesian Type</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>CMA ES-  —→ Genetic type; NSGA —&gt; Genetic type; MOTPE→ Genetic type</p>
<p><img src="content/Untitled%205.png" alt="content/Untitled%205.png"></p>
<p>Trick 1: sampler..</p>
<p>Trick 2: Distribution</p>
<p>Trick 3: Prune</p>
<p>Thumb rule says: - best pruner for random search is Median,</p>
<pre><code>                     best pruner for Bayesian opt is Hyperband
</code></pre>
<p><a href="https://medium.com/optuna/multivariate-tpe-makes-optuna-even-more-powerful-63c4bfbaebe2">&ldquo;Multivariate&rdquo; TPE Makes Optuna Even More Powerful</a></p>
<p><a href="https://youtu.be/-UeC4MR3PHM">https://youtu.be/-UeC4MR3PHM</a></p>
<p>could not able to finish it Today. Lend 1 hour from next day and finished. yea&hellip; made the commit around 1:07 next day.</p>
<p>Day 43:
Geo spatial analysis on the same predict future sales data sets.</p>
<p>extracted the cities.</p>
<p>awesome packages are</p>
<p>from geopy.exc import GeocoderTimedOut</p>
<p>from geopy.geocoders import Nominatim</p>
<p>not the best plots. but baby steps towards excellence</p>
<p><img src="content/Untitled%206.png" alt="content/Untitled%206.png"></p>
<p><img src="content/Untitled%207.png" alt="content/Untitled%207.png"></p>
<p>Day 44:
Holiday:</p>
<p>Brushing up machinelearning concepts:</p>
<p><a href="https://www.youtube.com/watch?v=mfzHchd5La8">https://www.youtube.com/watch?v=mfzHchd5La8</a></p>
<p>Day 45:</p>
<p>Grapext data analysis with visualisation and the data set is interesting Sedora Challenge</p>
<p>Day 46:</p>
<p>Build week Started&hellip;</p>
<p><a href="https://linuxize.com/post/how-to-install-and-use-docker-on-ubuntu-20-04/"></a></p>
<p><img src="content/Untitled%208.png" alt="content/Untitled%208.png"></p>
<p>Day 48:
Day 49:
Day 50: Build Week ended.</p>
<p>The most interesting week: <a href="https://github.com/Newton-Fitness/main/blob/master/newton-fitness-watch.ipynb">https://github.com/Newton-Fitness/main/blob/master/newton-fitness-watch.ipynb</a> very nice team.</p>
<p>Learned about the significance of the preprocessing.</p>
<p><strong>Deep learning</strong>:</p>
<p>Day 52: single neuron from scratch</p>
<p>epoch = one forward and backward pass of ALL training samples</p>
<p>batch_size = number of training samples used in one forward/backward pass</p>
<p>number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes</p>
<p>e.g : 100 samples, batch_size=20 -&gt; 100/20=5 iterations for 1 epoch</p>
<p>Day 53:</p>
<p><a href="https://www.asimovinstitute.org/neural-network-zoo/">https://www.asimovinstitute.org/neural-network-zoo/</a></p>
<p>Signal vs Noise:</p>
<p>The <strong>signal</strong> is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the <strong>noise</strong> is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can&rsquo;t actually help the model make predictions. The noise is the part might look useful but really isn&rsquo;t. <strong>Underfitting</strong> the training set is when the loss is not as low as it could be because the model hasn&rsquo;t learned enough signal. <strong>Overfitting</strong> the training set is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.</p>
<h1 id="capacity--important-in-neural-network">Capacity  (important in neural network)</h1>
<p>A model&rsquo;s <strong>capacity</strong> refers to the size and complexity of the patterns it is able to learn.
For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity. we can increase the capacity of a network either by making it <em>wider</em> (more units to existing layers) or by making it <em>deeper</em>  (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones.
Which is better just depends on the dataset.</p>
<p>Always try to use metrics like cross-entropy for a classification loss. 😇</p>
<p>Day 54:</p>
<p>Pytorch vs keras vs tensorflow..</p>
<p>Pytorch for research and good perfomance,</p>
<p>keras easy and average performance</p>
<p>tensorflow difficult and good performance</p>
<p>The process of finding the gradient descent for the complete data set is inefficient and instead we divide into small batches.</p>
<p>epoch = one forward and backward pass of ALL training samples</p>
<p>batch_size = number of training samples used in one forward/backward pass</p>
<p>number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples.</p>
<p>e.g : 100 samples, batch_size=20 -&gt; 100/20=5 iterations for 1 epoch</p>
<p>The learning rate controls how quickly the model is adapted to the problem. The amount that the weights are updated during training is referred to as the “learning rate.” [0.0 and 1.0.]</p>
<p>The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs. A learning rate that is too large can cause the model to converge too quickly to a sub-optimal solution, whereas a learning rate that is too small can cause the process to get stuck.</p>
<p>Day 55:</p>
<p>computer vision with tensorflow[keras]. image classifier with keras. Self interest learned from Kaggle.</p>
<p><strong>Input</strong> —&gt; <strong>Extract</strong> —&gt; <strong>Classify —&gt; Output</strong></p>
<p><em>Base</em>: Extract features from the input (images). <em>Head</em>: Match classified features with output classes.</p>
<p>There are pre-trained bases available:</p>
<ul>
<li><strong>VGG19()</strong>
<ul>
<li>Size: 549 MB</li>
<li>Top-1: Accuracy: 71.3%</li>
<li>Top-5: Accuracy: 90.0%</li>
<li>Number of Parameters: 143,667,240</li>
<li>Depth: 26</li>
</ul>
</li>
<li><strong>Inceptionv3</strong> (GoogLeNet)
<ul>
<li>Size: 92 MB</li>
<li>Top-1: Accuracy: 77.9%</li>
<li>Top-5: Accuracy: 93.7%</li>
<li>Number of Parameters: 23,851,784</li>
<li>Depth: 159</li>
</ul>
</li>
<li><strong>ResNet50</strong>
<ul>
<li>Size: 98 MB</li>
<li>Top-1: Accuracy: 74.9%</li>
<li>Top-5: Accuracy: 92.1%</li>
<li>Number of Parameters: 25,636,712</li>
</ul>
</li>
<li><strong>EfficientNet</strong>
<ul>
<li>Size: 29 MB</li>
<li>Top-1: Accuracy: 77.1%</li>
<li>Top-5: Accuracy: 93.3%</li>
<li>Number of Parameters: ~5,300,000</li>
<li>Depth: 159</li>
</ul>
</li>
</ul>
<p><a href="https://gist.githubusercontent.com/ogyalcin/052f2df49b3288e62086aa0e5fd25fcd/raw/9552fefe163908b383e581f5ff28c916289f2d37/Pre-Trained%20Model%20Performances.csv"></a></p>
<p>Is more epochs better accuracy?—&gt; No, it depends. unless the validation loss decreases one could do more epochs. when it increases then need to be stopped.</p>
<p>Day 56:  CNN</p>
<p>Why tensors instead of csv files&hellip;?</p>
<p>Neural networks don&rsquo;t process raw data, like text files, encoded JPEG image files, or CSV files. They process vectorized &amp; standardized representations.</p>
<ul>
<li>Text files need to be read into string tensors, then split into words. Finally, the words need to be indexed &amp; turned into integer tensors.</li>
<li>Images need to be read and decoded into integer tensors, then converted to floating point and normalized to small values (usually between 0 and 1).</li>
<li>CSV data needs to be parsed, with numerical features converted to floating point tensors and categorical features indexed and converted to integer tensors.
Then each feature typically needs to be normalized to zero-mean and unit-variance.</li>
</ul>
<p>Today working on FMNIST data by Zalando.</p>
<p>Day 57: RNN, LSTM</p>
<p><a href="https://youtu.be/c36lUUr864M">https://youtu.be/c36lUUr864M</a></p>
<p>Day 58: Custom dataset loader,</p>
<p><a href="https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f">Building Efficient Custom Datasets in PyTorch</a></p>
<p>Day 59: Transfer Learning:</p>
<p>when to use pretrained models and when not to use pretrained models?</p>
<p>Day 60: exercising with pytorch and optimizers</p>
<p>Dont blindly believe the Optimizer will do magics. Not true.. It completely depends upon the use case. for example.</p>
<p>Input : X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
expected output: Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)</p>
<p>weight: strating from 0</p>
<p>without optimizers:</p>
<p>loss—&gt;0 in 13 epochs. learning rate = 0.01, loss function = (y-y_pred).mean()</p>
<p>gradient: np.dot(2*x, y_pred - y).mean()</p>
<p>with SGD:</p>
<p>loss—&gt;0 in 30 epochs, loss function = nn.MSELoss()</p>
<p>with Adam:</p>
<p>loss — &gt;0.4 in 3000 epochs,  loss function = nn.MSELoss()</p>
<p><img src="content/Untitled%209.png" alt="content/Untitled%209.png"></p>
<p>In General,  Adam is mostly used in academic researches and it is fast. It combines the advantages of several other optimizer. SGD converges quickly.</p>
<p>Different types of criterion available in PyTorch:</p>
<p><a href="https://nn.readthedocs.io/en/latest/criterion/index.html">https://nn.readthedocs.io/en/latest/criterion/index.html</a></p>
<p>Different types of activate functions:</p>
<p><a href="https://pytorch.org/docs/stable/nn.functional.html">torch.nn.functional - PyTorch 1.8.1 documentation</a></p>
<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<h1 id="epoch--one-forward-and-backward-pass-of-all-training-samples">epoch = one forward and backward pass of ALL training samples</h1>
<h1 id="batch_size--number-of-training-samples-used-in-one-forwardbackward-pass">batch_size = number of training samples used in one forward/backward pass</h1>
<h1 id="number-of-iterations--number-of-passes-each-pass-forwardbackward-using-batch_size-number-of-sampes">number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes</h1>
<h1 id="eg--100-samples-batch_size20---100205-iterations-for-1-epoch">e.g : 100 samples, batch_size=20 -&gt; 100/20=5 iterations for 1 epoch</h1>
<p><img src="content/photo_2021-03-17_09-32-46.jpg" alt="content/photo_2021-03-17_09-32-46.jpg"></p>
<p><a href="https://www.notion.so">https://www.notion.so</a></p>
<p><img src="content/Untitled%2010.png" alt="content/Untitled%2010.png"></p>
<p><img src="content/photo_2021-03-17_09-32-46%201.jpg" alt="content/photo_2021-03-17_09-32-46%201.jpg"></p>
<p><img src="content/photo_2021-03-17_09-32-53.jpg" alt="content/photo_2021-03-17_09-32-53.jpg"></p>
<p><img src="content/photo_2021-03-17_09-32-55.jpg" alt="content/photo_2021-03-17_09-32-55.jpg"></p>
<p><img src="content/photo_2021-03-17_09-32-57.jpg" alt="content/photo_2021-03-17_09-32-57.jpg"></p>
<p><img src="content/photo_2021-03-17_09-32-58.jpg" alt="content/photo_2021-03-17_09-32-58.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-00.jpg" alt="content/photo_2021-03-17_09-33-00.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-01.jpg" alt="content/photo_2021-03-17_09-33-01.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-02.jpg" alt="content/photo_2021-03-17_09-33-02.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-04.jpg" alt="content/photo_2021-03-17_09-33-04.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-05.jpg" alt="content/photo_2021-03-17_09-33-05.jpg"></p>
<p><img src="content/photo_2021-03-17_09-33-08.jpg" alt="content/photo_2021-03-17_09-33-08.jpg"></p>
<p><a href="mailto:git@github.com">git@github.com</a>:alvarobartt/trendet.git</p>
<p><a href="https://nn.readthedocs.io/en/latest/criterion/index.html">https://nn.readthedocs.io/en/latest/criterion/index.html</a></p>

</article>



</html>
